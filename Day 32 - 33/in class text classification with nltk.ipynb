{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "45b29762-905d-476b-ad80-4dd299f8d258",
   "metadata": {},
   "source": [
    "### Text classification with nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "f44124c0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting nltkNote: you may need to restart the kernel to use updated packages.\n",
      "\n",
      "  Using cached nltk-3.9.1-py3-none-any.whl.metadata (2.9 kB)\n",
      "Collecting click (from nltk)\n",
      "  Using cached click-8.1.7-py3-none-any.whl.metadata (3.0 kB)\n",
      "Collecting joblib (from nltk)\n",
      "  Using cached joblib-1.4.2-py3-none-any.whl.metadata (5.4 kB)\n",
      "Collecting regex>=2021.8.3 (from nltk)\n",
      "  Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl.metadata (41 kB)\n",
      "     ---------------------------------------- 0.0/41.5 kB ? eta -:--:--\n",
      "     --------- ------------------------------ 10.2/41.5 kB ? eta -:--:--\n",
      "     ---------------------------- --------- 30.7/41.5 kB 325.1 kB/s eta 0:00:01\n",
      "     -------------------------------------- 41.5/41.5 kB 401.4 kB/s eta 0:00:00\n",
      "Collecting tqdm (from nltk)\n",
      "  Using cached tqdm-4.67.1-py3-none-any.whl.metadata (57 kB)\n",
      "Requirement already satisfied: colorama in c:\\users\\devid\\desktop\\ai data analytics bootcamp october batch\\testenv\\lib\\site-packages (from click->nltk) (0.4.6)\n",
      "Using cached nltk-3.9.1-py3-none-any.whl (1.5 MB)\n",
      "Downloading regex-2024.11.6-cp312-cp312-win_amd64.whl (273 kB)\n",
      "   ---------------------------------------- 0.0/273.6 kB ? eta -:--:--\n",
      "   -------- ------------------------------- 61.4/273.6 kB 1.7 MB/s eta 0:00:01\n",
      "   -------------------------------- ------- 225.3/273.6 kB 3.5 MB/s eta 0:00:01\n",
      "   ---------------------------------------- 273.6/273.6 kB 3.4 MB/s eta 0:00:00\n",
      "Using cached click-8.1.7-py3-none-any.whl (97 kB)\n",
      "Using cached joblib-1.4.2-py3-none-any.whl (301 kB)\n",
      "Using cached tqdm-4.67.1-py3-none-any.whl (78 kB)\n",
      "Installing collected packages: tqdm, regex, joblib, click, nltk\n",
      "Successfully installed click-8.1.7 joblib-1.4.2 nltk-3.9.1 regex-2024.11.6 tqdm-4.67.1\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "\n",
      "[notice] A new release of pip is available: 24.0 -> 24.3.1\n",
      "[notice] To update, run: python.exe -m pip install --upgrade pip\n"
     ]
    }
   ],
   "source": [
    "pip install nltk"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e568d273-9574-48db-bba6-1612d235e789",
   "metadata": {},
   "outputs": [],
   "source": [
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from nltk.tokenize import word_tokenize\n",
    "from nltk.classify import NaiveBayesClassifier"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "c99b03ea",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package movie_reviews to\n",
      "[nltk_data]     C:\\Users\\devid/nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\movie_reviews.zip.\n",
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\devid/nltk_data...\n",
      "[nltk_data]   Unzipping corpora\\stopwords.zip.\n",
      "[nltk_data] Downloading package punkt to C:\\Users\\devid/nltk_data...\n",
      "[nltk_data]   Package punkt is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Download necessary datasets\n",
    "nltk.download('movie_reviews')  # Movie review dataset\n",
    "nltk.download('stopwords')  # Stopwords for preprocessing\n",
    "nltk.download('punkt')     # Tokenizer for text processing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "aec9a398",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 1: Sample dataset of sentences labeled as positive or negative\n",
    "training_data = [\n",
    "    (\"I love this movie\", \"pos\"),\n",
    "    (\"This film is amazing\", \"pos\"),\n",
    "    (\"I hated this movie\", \"neg\"),\n",
    "    (\"This film is terrible\", \"neg\")\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "3682f917",
   "metadata": {},
   "outputs": [],
   "source": [
    "sentence = \"I love this movie\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "5e7c93e4",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 2: Preprocessing and Feature Extraction\n",
    "\n",
    "def extract_features(sentence):\n",
    "    words = word_tokenize(sentence.lower())  # Tokenize and convert to lowercase\n",
    "    stop_words = set(stopwords.words('english'))  # Get stop words\n",
    "    words = [word for word in words if word.isalpha() and word not in stop_words]  # Remove punctuation and stop words\n",
    "    return {word: True for word in words}  # Create feature dictionary with word presence as True"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "39d92b36",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'love': True, 'movie': True}"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "extract_features(sentence)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "dfcec11c",
   "metadata": {},
   "outputs": [],
   "source": [
    "def x():\n",
    "    a = 10\n",
    "    b = 20\n",
    "    c = a + b\n",
    "    return c\n",
    "\n",
    "# Function scope\n",
    "\n",
    "obj = x()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "35893893",
   "metadata": {},
   "outputs": [
    {
     "ename": "NameError",
     "evalue": "name 'c' is not defined",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mNameError\u001b[0m                                 Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[25], line 1\u001b[0m\n\u001b[1;32m----> 1\u001b[0m \u001b[43mc\u001b[49m\n",
      "\u001b[1;31mNameError\u001b[0m: name 'c' is not defined"
     ]
    }
   ],
   "source": [
    "c"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "id": "ed885e88",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "30\n"
     ]
    }
   ],
   "source": [
    "print(obj)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "id": "69008ce2",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Convert training data into feature sets\n",
    "training_features = [(extract_features(sentence), label) for sentence, label in training_data]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "id": "1acd2e24",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[({'love': True, 'movie': True}, 'pos'),\n",
       " ({'film': True, 'amazing': True}, 'pos'),\n",
       " ({'hated': True, 'movie': True}, 'neg'),\n",
       " ({'film': True, 'terrible': True}, 'neg')]"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "training_features"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "id": "a5f6f62b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 3: Train Naive Bayes Classifier\n",
    "classifier = NaiveBayesClassifier.train(training_features)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "id": "3c40181c",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Step 4: Test the classifier with new sentences\n",
    "test_sentences = [\n",
    "    \"I really enjoyed this movie\",\n",
    "    \"This movie I hated\"\n",
    "]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "id": "48ba6d0a",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Sentence: 'I really enjoyed this movie' => Predicted Sentiment: pos\n",
      "Sentence: 'This movie I hated' => Predicted Sentiment: neg\n"
     ]
    }
   ],
   "source": [
    "for sentence in test_sentences:\n",
    "    features = extract_features(sentence)\n",
    "    \n",
    "    predicted_label = classifier.classify(features)\n",
    "    \n",
    "    print(f\"Sentence: '{sentence}' => Predicted Sentiment: {predicted_label}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2d773ed0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# reviews for a product\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "eea630d0",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "186c1957",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "97f697d7",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "813d915f",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "testenv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
